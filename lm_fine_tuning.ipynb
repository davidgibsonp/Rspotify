{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lm_fine_tuning.ipynb",
      "provenance": [],
      "mount_file_id": "1uJ2SQdQINGoaYEn-wT4dovXeUtBYo7FA",
      "authorship_tag": "ABX9TyOf81EKYE4QP3svO8pBjYaF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidgibsonp/Rspotify/blob/master/lm_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOZAS8QPKEkv",
        "colab_type": "text"
      },
      "source": [
        "# Fine Tuning Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YivZia5KLXn",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF-4mrYxKb0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9PzCSv9KcoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVBMbg7hGqgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone tranformers and build from source\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "!cd content/transformers\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "!cd content/transformers/examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-6SMtVzHHrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelWithLMHead"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVWm9UN-KOzU",
        "colab_type": "text"
      },
      "source": [
        "## Fine-Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3AZPQYfG8v-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eca1d546-b47e-4e63-e197-3644231be340"
      },
      "source": [
        "!python language-modeling/run_language_modeling.py \\\n",
        "    --output_dir='/content/drive/My Drive/NLP/distilgpt2' \\\n",
        "    --cache_dir='/content/drive/My Drive/NLP/distilgpt2/cache' \\\n",
        "    --model_type=distilgpt2 \\\n",
        "    --model_name_or_path=distilgpt2 \\\n",
        "    --do_train \\\n",
        "    --train_data_file='/content/drive/My Drive/NLP/data/all_descriptions.txt' \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=1.0 \\\n",
        "    --do_train \\\n",
        "    --logging_steps=500 \\\n",
        "    --save_steps=500 \\\n",
        "    --per_gpu_train_batch_size=2 \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=128 \\\n",
        "    --gradient_accumulation_steps=5"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-19 19:03:44.749405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "06/19/2020 19:03:46 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "06/19/2020 19:03:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "06/19/2020 19:03:46 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/drive/My Drive/NLP/distilgpt2', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=2, gradient_accumulation_steps=5, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jun19_19-03-46_a58928ade4bc', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=5, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
            "06/19/2020 19:03:46 - INFO - filelock -   Lock 140350930884984 acquired on /content/drive/My Drive/NLP/distilgpt2/cache/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.7c30f0c070132208a64285a2dce903c733f1db6a70d16d1fac1663fd79b640b7.lock\n",
            "06/19/2020 19:03:46 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-config.json not found in cache or force_download set to True, downloading to /content/drive/My Drive/NLP/distilgpt2/cache/tmpkoza5dmo\n",
            "Downloading: 100% 762/762 [00:00<00:00, 661kB/s]\n",
            "06/19/2020 19:03:47 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-config.json in cache at /content/drive/My Drive/NLP/distilgpt2/cache/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.7c30f0c070132208a64285a2dce903c733f1db6a70d16d1fac1663fd79b640b7\n",
            "06/19/2020 19:03:47 - INFO - transformers.file_utils -   creating metadata file for /content/drive/My Drive/NLP/distilgpt2/cache/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.7c30f0c070132208a64285a2dce903c733f1db6a70d16d1fac1663fd79b640b7\n",
            "06/19/2020 19:03:47 - INFO - filelock -   Lock 140350930884984 released on /content/drive/My Drive/NLP/distilgpt2/cache/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.7c30f0c070132208a64285a2dce903c733f1db6a70d16d1fac1663fd79b640b7.lock\n",
            "06/19/2020 19:03:47 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-config.json from cache at /content/drive/My Drive/NLP/distilgpt2/cache/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.7c30f0c070132208a64285a2dce903c733f1db6a70d16d1fac1663fd79b640b7\n",
            "06/19/2020 19:03:47 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "06/19/2020 19:03:47 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-config.json from cache at /content/drive/My Drive/NLP/distilgpt2/cache/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.7c30f0c070132208a64285a2dce903c733f1db6a70d16d1fac1663fd79b640b7\n",
            "06/19/2020 19:03:47 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "06/19/2020 19:03:48 - INFO - filelock -   Lock 140350930884032 acquired on /content/drive/My Drive/NLP/distilgpt2/cache/71cc2431cf0b5bbe7a23601a808ed322c90251c8261b46f04970140a3c2c1cb4.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "06/19/2020 19:03:48 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-vocab.json not found in cache or force_download set to True, downloading to /content/drive/My Drive/NLP/distilgpt2/cache/tmp1385v0my\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 1.77MB/s]\n",
            "06/19/2020 19:03:49 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-vocab.json in cache at /content/drive/My Drive/NLP/distilgpt2/cache/71cc2431cf0b5bbe7a23601a808ed322c90251c8261b46f04970140a3c2c1cb4.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "06/19/2020 19:03:49 - INFO - transformers.file_utils -   creating metadata file for /content/drive/My Drive/NLP/distilgpt2/cache/71cc2431cf0b5bbe7a23601a808ed322c90251c8261b46f04970140a3c2c1cb4.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "06/19/2020 19:03:49 - INFO - filelock -   Lock 140350930884032 released on /content/drive/My Drive/NLP/distilgpt2/cache/71cc2431cf0b5bbe7a23601a808ed322c90251c8261b46f04970140a3c2c1cb4.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "06/19/2020 19:03:49 - INFO - filelock -   Lock 140350931299408 acquired on /content/drive/My Drive/NLP/distilgpt2/cache/4faf7afb02a1ea7d2944e9ba7a175c7b8de4957cdbae75cd5ddffc7c7643ebbc.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "06/19/2020 19:03:49 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-merges.txt not found in cache or force_download set to True, downloading to /content/drive/My Drive/NLP/distilgpt2/cache/tmpgg_p2ak5\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.15MB/s]\n",
            "06/19/2020 19:03:50 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-merges.txt in cache at /content/drive/My Drive/NLP/distilgpt2/cache/4faf7afb02a1ea7d2944e9ba7a175c7b8de4957cdbae75cd5ddffc7c7643ebbc.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "06/19/2020 19:03:50 - INFO - transformers.file_utils -   creating metadata file for /content/drive/My Drive/NLP/distilgpt2/cache/4faf7afb02a1ea7d2944e9ba7a175c7b8de4957cdbae75cd5ddffc7c7643ebbc.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "06/19/2020 19:03:50 - INFO - filelock -   Lock 140350931299408 released on /content/drive/My Drive/NLP/distilgpt2/cache/4faf7afb02a1ea7d2944e9ba7a175c7b8de4957cdbae75cd5ddffc7c7643ebbc.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "06/19/2020 19:03:50 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-vocab.json from cache at /content/drive/My Drive/NLP/distilgpt2/cache/71cc2431cf0b5bbe7a23601a808ed322c90251c8261b46f04970140a3c2c1cb4.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "06/19/2020 19:03:50 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-merges.txt from cache at /content/drive/My Drive/NLP/distilgpt2/cache/4faf7afb02a1ea7d2944e9ba7a175c7b8de4957cdbae75cd5ddffc7c7643ebbc.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:774: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "06/19/2020 19:03:50 - INFO - filelock -   Lock 140350931299744 acquired on /content/drive/My Drive/NLP/distilgpt2/cache/cd250f30004d0dee11ff1af311bd3facb6f38739fd870b9c8aa9321333a550be.ffe4c53a2a410b15148cf4170cc408d2d2f98adeecdde146ef8e71843039ff3c.lock\n",
            "06/19/2020 19:03:50 - INFO - transformers.file_utils -   https://cdn.huggingface.co/distilgpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to /content/drive/My Drive/NLP/distilgpt2/cache/tmppukt2o39\n",
            "Downloading: 100% 353M/353M [00:10<00:00, 33.6MB/s]\n",
            "06/19/2020 19:04:01 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/distilgpt2-pytorch_model.bin in cache at /content/drive/My Drive/NLP/distilgpt2/cache/cd250f30004d0dee11ff1af311bd3facb6f38739fd870b9c8aa9321333a550be.ffe4c53a2a410b15148cf4170cc408d2d2f98adeecdde146ef8e71843039ff3c\n",
            "06/19/2020 19:04:01 - INFO - transformers.file_utils -   creating metadata file for /content/drive/My Drive/NLP/distilgpt2/cache/cd250f30004d0dee11ff1af311bd3facb6f38739fd870b9c8aa9321333a550be.ffe4c53a2a410b15148cf4170cc408d2d2f98adeecdde146ef8e71843039ff3c\n",
            "06/19/2020 19:04:01 - INFO - filelock -   Lock 140350931299744 released on /content/drive/My Drive/NLP/distilgpt2/cache/cd250f30004d0dee11ff1af311bd3facb6f38739fd870b9c8aa9321333a550be.ffe4c53a2a410b15148cf4170cc408d2d2f98adeecdde146ef8e71843039ff3c.lock\n",
            "06/19/2020 19:04:01 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/distilgpt2-pytorch_model.bin from cache at /content/drive/My Drive/NLP/distilgpt2/cache/cd250f30004d0dee11ff1af311bd3facb6f38739fd870b9c8aa9321333a550be.ffe4c53a2a410b15148cf4170cc408d2d2f98adeecdde146ef8e71843039ff3c\n",
            "06/19/2020 19:04:05 - INFO - transformers.modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias']\n",
            "06/19/2020 19:04:05 - INFO - filelock -   Lock 140350779355264 acquired on /content/drive/My Drive/NLP/data/cached_lm_GPT2Tokenizer_128_all_descriptions.txt.lock\n",
            "06/19/2020 19:04:05 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at /content/drive/My Drive/NLP/data\n",
            "06/19/2020 19:04:06 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file /content/drive/My Drive/NLP/data/cached_lm_GPT2Tokenizer_128_all_descriptions.txt [took 0.008 s]\n",
            "06/19/2020 19:04:06 - INFO - filelock -   Lock 140350779355264 released on /content/drive/My Drive/NLP/data/cached_lm_GPT2Tokenizer_128_all_descriptions.txt.lock\n",
            "06/19/2020 19:04:23 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "06/19/2020 19:04:23 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "06/19/2020 19:04:23 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "06/19/2020 19:04:23 - INFO - transformers.trainer -   ***** Running training *****\n",
            "06/19/2020 19:04:23 - INFO - transformers.trainer -     Num examples = 1394\n",
            "06/19/2020 19:04:23 - INFO - transformers.trainer -     Num Epochs = 1\n",
            "06/19/2020 19:04:23 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "06/19/2020 19:04:23 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "06/19/2020 19:04:23 - INFO - transformers.trainer -     Gradient Accumulation steps = 5\n",
            "06/19/2020 19:04:23 - INFO - transformers.trainer -     Total optimization steps = 139\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/697 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/697 [00:00<02:09,  5.36it/s]\u001b[A\n",
            "Iteration:   0% 3/697 [00:00<01:43,  6.68it/s]\u001b[A\n",
            "Iteration:   1% 5/697 [00:00<01:43,  6.69it/s]\u001b[A\n",
            "Iteration:   1% 7/697 [00:00<01:22,  8.33it/s]\u001b[A\n",
            "Iteration:   1% 10/697 [00:00<01:08, 10.05it/s]\u001b[A\n",
            "Iteration:   2% 12/697 [00:00<00:59, 11.58it/s]\u001b[A\n",
            "Iteration:   2% 15/697 [00:01<00:52, 13.04it/s]\u001b[A\n",
            "Iteration:   2% 17/697 [00:01<00:47, 14.19it/s]\u001b[A\n",
            "Iteration:   3% 20/697 [00:01<00:44, 15.14it/s]\u001b[A\n",
            "Iteration:   3% 22/697 [00:01<00:42, 15.94it/s]\u001b[A\n",
            "Iteration:   3% 24/697 [00:01<00:40, 16.63it/s]\u001b[A\n",
            "Iteration:   4% 26/697 [00:01<00:41, 16.29it/s]\u001b[A\n",
            "Iteration:   4% 29/697 [00:01<00:38, 17.34it/s]\u001b[A\n",
            "Iteration:   4% 31/697 [00:02<00:40, 16.39it/s]\u001b[A\n",
            "Iteration:   5% 33/697 [00:02<00:38, 17.24it/s]\u001b[A\n",
            "Iteration:   5% 35/697 [00:02<00:39, 16.83it/s]\u001b[A\n",
            "Iteration:   5% 37/697 [00:02<00:38, 17.04it/s]\u001b[A\n",
            "Iteration:   6% 39/697 [00:02<00:37, 17.78it/s]\u001b[A\n",
            "Iteration:   6% 41/697 [00:02<00:39, 16.69it/s]\u001b[A\n",
            "Iteration:   6% 43/697 [00:02<00:37, 17.31it/s]\u001b[A\n",
            "Iteration:   6% 45/697 [00:02<00:37, 17.27it/s]\u001b[A\n",
            "Iteration:   7% 47/697 [00:02<00:37, 17.26it/s]\u001b[A\n",
            "Iteration:   7% 49/697 [00:03<00:36, 17.72it/s]\u001b[A\n",
            "Iteration:   7% 51/697 [00:03<00:38, 16.64it/s]\u001b[A\n",
            "Iteration:   8% 53/697 [00:03<00:36, 17.46it/s]\u001b[A\n",
            "Iteration:   8% 55/697 [00:03<00:37, 16.95it/s]\u001b[A\n",
            "Iteration:   8% 57/697 [00:03<00:37, 17.16it/s]\u001b[A\n",
            "Iteration:   8% 59/697 [00:03<00:35, 17.83it/s]\u001b[A\n",
            "Iteration:   9% 61/697 [00:03<00:37, 16.94it/s]\u001b[A\n",
            "Iteration:   9% 63/697 [00:03<00:35, 17.73it/s]\u001b[A\n",
            "Iteration:   9% 65/697 [00:04<00:37, 17.02it/s]\u001b[A\n",
            "Iteration:  10% 67/697 [00:04<00:35, 17.52it/s]\u001b[A\n",
            "Iteration:  10% 69/697 [00:04<00:34, 18.01it/s]\u001b[A\n",
            "Iteration:  10% 71/697 [00:04<00:37, 16.87it/s]\u001b[A\n",
            "Iteration:  10% 73/697 [00:04<00:35, 17.49it/s]\u001b[A\n",
            "Iteration:  11% 75/697 [00:04<00:35, 17.30it/s]\u001b[A\n",
            "Iteration:  11% 77/697 [00:04<00:36, 17.18it/s]\u001b[A\n",
            "Iteration:  11% 80/697 [00:04<00:35, 17.25it/s]\u001b[A\n",
            "Iteration:  12% 82/697 [00:04<00:35, 17.43it/s]\u001b[A\n",
            "Iteration:  12% 84/697 [00:05<00:34, 18.02it/s]\u001b[A\n",
            "Iteration:  12% 86/697 [00:05<00:35, 17.17it/s]\u001b[A\n",
            "Iteration:  13% 89/697 [00:05<00:33, 17.93it/s]\u001b[A\n",
            "Iteration:  13% 91/697 [00:05<00:35, 16.96it/s]\u001b[A\n",
            "Iteration:  13% 93/697 [00:05<00:34, 17.54it/s]\u001b[A\n",
            "Iteration:  14% 95/697 [00:05<00:35, 16.94it/s]\u001b[A\n",
            "Iteration:  14% 97/697 [00:05<00:35, 17.02it/s]\u001b[A\n",
            "Iteration:  14% 100/697 [00:06<00:34, 17.12it/s]\u001b[A\n",
            "Iteration:  15% 102/697 [00:06<00:34, 17.34it/s]\u001b[A\n",
            "Iteration:  15% 104/697 [00:06<00:33, 17.96it/s]\u001b[A\n",
            "Iteration:  15% 106/697 [00:06<00:34, 16.89it/s]\u001b[A\n",
            "Iteration:  15% 108/697 [00:06<00:33, 17.64it/s]\u001b[A\n",
            "Iteration:  16% 110/697 [00:06<00:33, 17.50it/s]\u001b[A\n",
            "Iteration:  16% 112/697 [00:06<00:33, 17.55it/s]\u001b[A\n",
            "Iteration:  16% 114/697 [00:06<00:32, 18.22it/s]\u001b[A\n",
            "Iteration:  17% 116/697 [00:06<00:33, 17.53it/s]\u001b[A\n",
            "Iteration:  17% 119/697 [00:07<00:31, 18.45it/s]\u001b[A\n",
            "Iteration:  17% 121/697 [00:07<00:32, 17.79it/s]\u001b[A\n",
            "Iteration:  18% 124/697 [00:07<00:30, 18.71it/s]\u001b[A\n",
            "Iteration:  18% 126/697 [00:07<00:31, 17.95it/s]\u001b[A\n",
            "Iteration:  19% 129/697 [00:07<00:30, 18.82it/s]\u001b[A\n",
            "Iteration:  19% 131/697 [00:07<00:31, 18.01it/s]\u001b[A\n",
            "Iteration:  19% 134/697 [00:07<00:29, 18.84it/s]\u001b[A\n",
            "Iteration:  20% 136/697 [00:07<00:31, 18.02it/s]\u001b[A\n",
            "Iteration:  20% 139/697 [00:08<00:29, 18.88it/s]\u001b[A\n",
            "Iteration:  20% 141/697 [00:08<00:30, 18.07it/s]\u001b[A\n",
            "Iteration:  21% 144/697 [00:08<00:29, 18.94it/s]\u001b[A\n",
            "Iteration:  21% 146/697 [00:08<00:30, 18.06it/s]\u001b[A\n",
            "Iteration:  21% 149/697 [00:08<00:28, 18.92it/s]\u001b[A\n",
            "Iteration:  22% 151/697 [00:08<00:30, 18.09it/s]\u001b[A\n",
            "Iteration:  22% 154/697 [00:08<00:28, 18.89it/s]\u001b[A\n",
            "Iteration:  22% 156/697 [00:09<00:29, 18.09it/s]\u001b[A\n",
            "Iteration:  23% 159/697 [00:09<00:28, 18.94it/s]\u001b[A\n",
            "Iteration:  23% 161/697 [00:09<00:29, 18.08it/s]\u001b[A\n",
            "Iteration:  24% 164/697 [00:09<00:28, 18.89it/s]\u001b[A\n",
            "Iteration:  24% 166/697 [00:09<00:29, 18.04it/s]\u001b[A\n",
            "Iteration:  24% 169/697 [00:09<00:27, 18.87it/s]\u001b[A\n",
            "Iteration:  25% 171/697 [00:09<00:29, 18.00it/s]\u001b[A\n",
            "Iteration:  25% 174/697 [00:09<00:27, 18.90it/s]\u001b[A\n",
            "Iteration:  25% 176/697 [00:10<00:28, 18.01it/s]\u001b[A\n",
            "Iteration:  26% 179/697 [00:10<00:27, 18.86it/s]\u001b[A\n",
            "Iteration:  26% 181/697 [00:10<00:28, 18.00it/s]\u001b[A\n",
            "Iteration:  26% 184/697 [00:10<00:27, 18.88it/s]\u001b[A\n",
            "Iteration:  27% 186/697 [00:10<00:28, 18.04it/s]\u001b[A\n",
            "Iteration:  27% 189/697 [00:10<00:26, 18.88it/s]\u001b[A\n",
            "Iteration:  27% 191/697 [00:10<00:27, 18.08it/s]\u001b[A\n",
            "Iteration:  28% 194/697 [00:11<00:26, 18.91it/s]\u001b[A\n",
            "Iteration:  28% 196/697 [00:11<00:27, 18.08it/s]\u001b[A\n",
            "Iteration:  29% 199/697 [00:11<00:26, 18.92it/s]\u001b[A\n",
            "Iteration:  29% 201/697 [00:11<00:27, 18.07it/s]\u001b[A\n",
            "Iteration:  29% 204/697 [00:11<00:26, 18.94it/s]\u001b[A\n",
            "Iteration:  30% 206/697 [00:11<00:27, 18.03it/s]\u001b[A\n",
            "Iteration:  30% 209/697 [00:11<00:25, 18.85it/s]\u001b[A\n",
            "Iteration:  30% 211/697 [00:11<00:26, 18.02it/s]\u001b[A\n",
            "Iteration:  31% 214/697 [00:12<00:25, 18.91it/s]\u001b[A\n",
            "Iteration:  31% 216/697 [00:12<00:26, 18.04it/s]\u001b[A\n",
            "Iteration:  31% 219/697 [00:12<00:25, 18.92it/s]\u001b[A\n",
            "Iteration:  32% 221/697 [00:12<00:26, 18.05it/s]\u001b[A\n",
            "Iteration:  32% 224/697 [00:12<00:25, 18.92it/s]\u001b[A\n",
            "Iteration:  32% 226/697 [00:12<00:26, 18.07it/s]\u001b[A\n",
            "Iteration:  33% 229/697 [00:12<00:24, 18.93it/s]\u001b[A\n",
            "Iteration:  33% 231/697 [00:12<00:25, 18.08it/s]\u001b[A\n",
            "Iteration:  34% 234/697 [00:13<00:24, 18.86it/s]\u001b[A\n",
            "Iteration:  34% 236/697 [00:13<00:25, 18.07it/s]\u001b[A\n",
            "Iteration:  34% 239/697 [00:13<00:24, 18.88it/s]\u001b[A\n",
            "Iteration:  35% 241/697 [00:13<00:25, 18.05it/s]\u001b[A\n",
            "Iteration:  35% 244/697 [00:13<00:24, 18.87it/s]\u001b[A\n",
            "Iteration:  35% 246/697 [00:13<00:25, 18.03it/s]\u001b[A\n",
            "Iteration:  36% 249/697 [00:13<00:23, 18.85it/s]\u001b[A\n",
            "Iteration:  36% 251/697 [00:14<00:24, 18.07it/s]\u001b[A\n",
            "Iteration:  36% 254/697 [00:14<00:23, 18.89it/s]\u001b[A\n",
            "Iteration:  37% 256/697 [00:14<00:24, 17.93it/s]\u001b[A\n",
            "Iteration:  37% 259/697 [00:14<00:23, 18.71it/s]\u001b[A\n",
            "Iteration:  37% 261/697 [00:14<00:24, 17.93it/s]\u001b[A\n",
            "Iteration:  38% 264/697 [00:14<00:23, 18.72it/s]\u001b[A\n",
            "Iteration:  38% 266/697 [00:14<00:24, 17.95it/s]\u001b[A\n",
            "Iteration:  39% 269/697 [00:14<00:22, 18.83it/s]\u001b[A\n",
            "Iteration:  39% 271/697 [00:15<00:23, 17.95it/s]\u001b[A\n",
            "Iteration:  39% 274/697 [00:15<00:22, 18.76it/s]\u001b[A\n",
            "Iteration:  40% 276/697 [00:15<00:23, 17.96it/s]\u001b[A\n",
            "Iteration:  40% 279/697 [00:15<00:22, 18.78it/s]\u001b[A\n",
            "Iteration:  40% 281/697 [00:15<00:23, 17.95it/s]\u001b[A\n",
            "Iteration:  41% 284/697 [00:15<00:21, 18.79it/s]\u001b[A\n",
            "Iteration:  41% 286/697 [00:15<00:22, 17.95it/s]\u001b[A\n",
            "Iteration:  41% 289/697 [00:16<00:21, 18.81it/s]\u001b[A\n",
            "Iteration:  42% 291/697 [00:16<00:22, 18.00it/s]\u001b[A\n",
            "Iteration:  42% 294/697 [00:16<00:21, 18.88it/s]\u001b[A\n",
            "Iteration:  42% 296/697 [00:16<00:22, 18.03it/s]\u001b[A\n",
            "Iteration:  43% 299/697 [00:16<00:21, 18.83it/s]\u001b[A\n",
            "Iteration:  43% 301/697 [00:16<00:21, 18.03it/s]\u001b[A\n",
            "Iteration:  44% 304/697 [00:16<00:20, 18.86it/s]\u001b[A\n",
            "Iteration:  44% 306/697 [00:16<00:21, 18.03it/s]\u001b[A\n",
            "Iteration:  44% 309/697 [00:17<00:20, 18.84it/s]\u001b[A\n",
            "Iteration:  45% 311/697 [00:17<00:21, 18.01it/s]\u001b[A\n",
            "Iteration:  45% 314/697 [00:17<00:20, 18.82it/s]\u001b[A\n",
            "Iteration:  45% 316/697 [00:17<00:21, 18.04it/s]\u001b[A\n",
            "Iteration:  46% 319/697 [00:17<00:20, 18.87it/s]\u001b[A\n",
            "Iteration:  46% 321/697 [00:17<00:20, 17.99it/s]\u001b[A\n",
            "Iteration:  46% 324/697 [00:17<00:19, 18.80it/s]\u001b[A\n",
            "Iteration:  47% 326/697 [00:18<00:20, 17.96it/s]\u001b[A\n",
            "Iteration:  47% 329/697 [00:18<00:19, 18.74it/s]\u001b[A\n",
            "Iteration:  47% 331/697 [00:18<00:20, 17.93it/s]\u001b[A\n",
            "Iteration:  48% 334/697 [00:18<00:19, 18.76it/s]\u001b[A\n",
            "Iteration:  48% 336/697 [00:18<00:20, 17.89it/s]\u001b[A\n",
            "Iteration:  49% 339/697 [00:18<00:19, 18.73it/s]\u001b[A\n",
            "Iteration:  49% 341/697 [00:18<00:19, 17.92it/s]\u001b[A\n",
            "Iteration:  49% 344/697 [00:18<00:18, 18.74it/s]\u001b[A\n",
            "Iteration:  50% 346/697 [00:19<00:19, 17.84it/s]\u001b[A\n",
            "Iteration:  50% 349/697 [00:19<00:18, 18.70it/s]\u001b[A\n",
            "Iteration:  50% 351/697 [00:19<00:19, 17.90it/s]\u001b[A\n",
            "Iteration:  51% 354/697 [00:19<00:18, 18.72it/s]\u001b[A\n",
            "Iteration:  51% 356/697 [00:19<00:19, 17.92it/s]\u001b[A\n",
            "Iteration:  52% 359/697 [00:19<00:18, 18.76it/s]\u001b[A\n",
            "Iteration:  52% 361/697 [00:19<00:18, 17.84it/s]\u001b[A\n",
            "Iteration:  52% 364/697 [00:20<00:17, 18.64it/s]\u001b[A\n",
            "Iteration:  53% 366/697 [00:20<00:18, 17.82it/s]\u001b[A\n",
            "Iteration:  53% 369/697 [00:20<00:17, 18.66it/s]\u001b[A\n",
            "Iteration:  53% 371/697 [00:20<00:18, 17.90it/s]\u001b[A\n",
            "Iteration:  54% 374/697 [00:20<00:17, 18.71it/s]\u001b[A\n",
            "Iteration:  54% 376/697 [00:20<00:17, 17.84it/s]\u001b[A\n",
            "Iteration:  54% 379/697 [00:20<00:17, 18.66it/s]\u001b[A\n",
            "Iteration:  55% 381/697 [00:20<00:17, 17.84it/s]\u001b[A\n",
            "Iteration:  55% 384/697 [00:21<00:16, 18.68it/s]\u001b[A\n",
            "Iteration:  55% 386/697 [00:21<00:17, 17.93it/s]\u001b[A\n",
            "Iteration:  56% 389/697 [00:21<00:16, 18.72it/s]\u001b[A\n",
            "Iteration:  56% 391/697 [00:21<00:17, 17.92it/s]\u001b[A\n",
            "Iteration:  57% 394/697 [00:21<00:16, 18.74it/s]\u001b[A\n",
            "Iteration:  57% 396/697 [00:21<00:16, 17.95it/s]\u001b[A\n",
            "Iteration:  57% 399/697 [00:21<00:15, 18.72it/s]\u001b[A\n",
            "Iteration:  58% 401/697 [00:22<00:16, 17.92it/s]\u001b[A\n",
            "Iteration:  58% 404/697 [00:22<00:15, 18.70it/s]\u001b[A\n",
            "Iteration:  58% 406/697 [00:22<00:16, 17.80it/s]\u001b[A\n",
            "Iteration:  59% 409/697 [00:22<00:15, 18.63it/s]\u001b[A\n",
            "Iteration:  59% 411/697 [00:22<00:16, 17.87it/s]\u001b[A\n",
            "Iteration:  59% 414/697 [00:22<00:15, 18.71it/s]\u001b[A\n",
            "Iteration:  60% 416/697 [00:22<00:15, 17.93it/s]\u001b[A\n",
            "Iteration:  60% 419/697 [00:22<00:14, 18.73it/s]\u001b[A\n",
            "Iteration:  60% 421/697 [00:23<00:15, 17.74it/s]\u001b[A\n",
            "Iteration:  61% 424/697 [00:23<00:14, 18.57it/s]\u001b[A\n",
            "Iteration:  61% 426/697 [00:23<00:15, 17.77it/s]\u001b[A\n",
            "Iteration:  62% 429/697 [00:23<00:14, 18.63it/s]\u001b[A\n",
            "Iteration:  62% 431/697 [00:23<00:14, 17.83it/s]\u001b[A\n",
            "Iteration:  62% 434/697 [00:23<00:14, 18.60it/s]\u001b[A\n",
            "Iteration:  63% 436/697 [00:23<00:14, 17.73it/s]\u001b[A\n",
            "Iteration:  63% 439/697 [00:24<00:13, 18.53it/s]\u001b[A\n",
            "Iteration:  63% 441/697 [00:24<00:14, 17.81it/s]\u001b[A\n",
            "Iteration:  64% 444/697 [00:24<00:13, 18.64it/s]\u001b[A\n",
            "Iteration:  64% 446/697 [00:24<00:14, 17.83it/s]\u001b[A\n",
            "Iteration:  64% 449/697 [00:24<00:13, 18.62it/s]\u001b[A\n",
            "Iteration:  65% 451/697 [00:24<00:13, 17.79it/s]\u001b[A\n",
            "Iteration:  65% 454/697 [00:24<00:13, 18.60it/s]\u001b[A\n",
            "Iteration:  65% 456/697 [00:24<00:13, 17.80it/s]\u001b[A\n",
            "Iteration:  66% 459/697 [00:25<00:12, 18.64it/s]\u001b[A\n",
            "Iteration:  66% 461/697 [00:25<00:13, 17.87it/s]\u001b[A\n",
            "Iteration:  67% 464/697 [00:25<00:12, 18.69it/s]\u001b[A\n",
            "Iteration:  67% 466/697 [00:25<00:12, 17.90it/s]\u001b[A\n",
            "Iteration:  67% 469/697 [00:25<00:12, 18.66it/s]\u001b[A\n",
            "Iteration:  68% 471/697 [00:25<00:12, 17.81it/s]\u001b[A\n",
            "Iteration:  68% 474/697 [00:25<00:12, 18.56it/s]\u001b[A\n",
            "Iteration:  68% 476/697 [00:26<00:12, 17.79it/s]\u001b[A\n",
            "Iteration:  69% 479/697 [00:26<00:11, 18.60it/s]\u001b[A\n",
            "Iteration:  69% 481/697 [00:26<00:12, 17.83it/s]\u001b[A\n",
            "Iteration:  69% 484/697 [00:26<00:11, 18.64it/s]\u001b[A\n",
            "Iteration:  70% 486/697 [00:26<00:11, 17.87it/s]\u001b[A\n",
            "Iteration:  70% 489/697 [00:26<00:11, 18.69it/s]\u001b[A\n",
            "Iteration:  70% 491/697 [00:26<00:11, 17.91it/s]\u001b[A\n",
            "Iteration:  71% 494/697 [00:26<00:10, 18.58it/s]\u001b[A\n",
            "Iteration:  71% 496/697 [00:27<00:11, 17.79it/s]\u001b[A\n",
            "Iteration:  72% 499/697 [00:27<00:10, 18.60it/s]\u001b[A\n",
            "Iteration:  72% 501/697 [00:27<00:11, 17.78it/s]\u001b[A\n",
            "Iteration:  72% 504/697 [00:27<00:10, 18.59it/s]\u001b[A\n",
            "Iteration:  73% 506/697 [00:27<00:10, 17.80it/s]\u001b[A\n",
            "Iteration:  73% 509/697 [00:27<00:10, 18.67it/s]\u001b[A\n",
            "Iteration:  73% 511/697 [00:27<00:10, 17.83it/s]\u001b[A\n",
            "Iteration:  74% 514/697 [00:28<00:09, 18.59it/s]\u001b[A\n",
            "Iteration:  74% 516/697 [00:28<00:10, 17.72it/s]\u001b[A\n",
            "Iteration:  74% 519/697 [00:28<00:09, 18.56it/s]\u001b[A\n",
            "Iteration:  75% 521/697 [00:28<00:09, 17.77it/s]\u001b[A\n",
            "Iteration:  75% 524/697 [00:28<00:09, 18.55it/s]\u001b[A\n",
            "Iteration:  75% 526/697 [00:28<00:09, 17.73it/s]\u001b[A\n",
            "Iteration:  76% 529/697 [00:28<00:09, 18.58it/s]\u001b[A\n",
            "Iteration:  76% 531/697 [00:28<00:09, 17.81it/s]\u001b[A\n",
            "Iteration:  77% 534/697 [00:29<00:08, 18.55it/s]\u001b[A\n",
            "Iteration:  77% 536/697 [00:29<00:09, 17.73it/s]\u001b[A\n",
            "Iteration:  77% 539/697 [00:29<00:08, 18.58it/s]\u001b[A\n",
            "Iteration:  78% 541/697 [00:29<00:08, 17.80it/s]\u001b[A\n",
            "Iteration:  78% 544/697 [00:29<00:08, 18.59it/s]\u001b[A\n",
            "Iteration:  78% 546/697 [00:29<00:08, 17.80it/s]\u001b[A\n",
            "Iteration:  79% 549/697 [00:29<00:07, 18.63it/s]\u001b[A\n",
            "Iteration:  79% 551/697 [00:30<00:08, 17.87it/s]\u001b[A\n",
            "Iteration:  79% 554/697 [00:30<00:07, 18.68it/s]\u001b[A\n",
            "Iteration:  80% 556/697 [00:30<00:07, 17.87it/s]\u001b[A\n",
            "Iteration:  80% 559/697 [00:30<00:07, 18.63it/s]\u001b[A\n",
            "Iteration:  80% 561/697 [00:30<00:07, 17.81it/s]\u001b[A\n",
            "Iteration:  81% 564/697 [00:30<00:07, 18.56it/s]\u001b[A\n",
            "Iteration:  81% 566/697 [00:30<00:07, 17.76it/s]\u001b[A\n",
            "Iteration:  82% 569/697 [00:31<00:06, 18.58it/s]\u001b[A\n",
            "Iteration:  82% 571/697 [00:31<00:07, 17.80it/s]\u001b[A\n",
            "Iteration:  82% 574/697 [00:31<00:06, 18.60it/s]\u001b[A\n",
            "Iteration:  83% 576/697 [00:31<00:06, 17.83it/s]\u001b[A\n",
            "Iteration:  83% 579/697 [00:31<00:06, 18.64it/s]\u001b[A\n",
            "Iteration:  83% 581/697 [00:31<00:06, 17.84it/s]\u001b[A\n",
            "Iteration:  84% 584/697 [00:31<00:06, 18.60it/s]\u001b[A\n",
            "Iteration:  84% 586/697 [00:31<00:06, 17.82it/s]\u001b[A\n",
            "Iteration:  85% 589/697 [00:32<00:05, 18.61it/s]\u001b[A\n",
            "Iteration:  85% 591/697 [00:32<00:05, 17.81it/s]\u001b[A\n",
            "Iteration:  85% 594/697 [00:32<00:05, 18.61it/s]\u001b[A\n",
            "Iteration:  86% 596/697 [00:32<00:05, 17.85it/s]\u001b[A\n",
            "Iteration:  86% 599/697 [00:32<00:05, 18.65it/s]\u001b[A\n",
            "Iteration:  86% 601/697 [00:32<00:05, 17.84it/s]\u001b[A\n",
            "Iteration:  87% 604/697 [00:32<00:05, 18.59it/s]\u001b[A\n",
            "Iteration:  87% 606/697 [00:33<00:05, 17.79it/s]\u001b[A\n",
            "Iteration:  87% 609/697 [00:33<00:04, 18.48it/s]\u001b[A\n",
            "Iteration:  88% 611/697 [00:33<00:04, 17.63it/s]\u001b[A\n",
            "Iteration:  88% 614/697 [00:33<00:04, 18.48it/s]\u001b[A\n",
            "Iteration:  88% 616/697 [00:33<00:04, 17.72it/s]\u001b[A\n",
            "Iteration:  89% 619/697 [00:33<00:04, 18.47it/s]\u001b[A\n",
            "Iteration:  89% 621/697 [00:33<00:04, 17.74it/s]\u001b[A\n",
            "Iteration:  90% 624/697 [00:33<00:03, 18.50it/s]\u001b[A\n",
            "Iteration:  90% 626/697 [00:34<00:04, 17.74it/s]\u001b[A\n",
            "Iteration:  90% 629/697 [00:34<00:03, 18.54it/s]\u001b[A\n",
            "Iteration:  91% 631/697 [00:34<00:03, 17.77it/s]\u001b[A\n",
            "Iteration:  91% 634/697 [00:34<00:03, 18.61it/s]\u001b[A\n",
            "Iteration:  91% 636/697 [00:34<00:03, 17.78it/s]\u001b[A\n",
            "Iteration:  92% 639/697 [00:34<00:03, 18.54it/s]\u001b[A\n",
            "Iteration:  92% 641/697 [00:34<00:03, 17.77it/s]\u001b[A\n",
            "Iteration:  92% 644/697 [00:35<00:02, 18.60it/s]\u001b[A\n",
            "Iteration:  93% 646/697 [00:35<00:02, 17.70it/s]\u001b[A\n",
            "Iteration:  93% 649/697 [00:35<00:02, 18.51it/s]\u001b[A\n",
            "Iteration:  93% 651/697 [00:35<00:02, 17.72it/s]\u001b[A\n",
            "Iteration:  94% 654/697 [00:35<00:02, 18.46it/s]\u001b[A\n",
            "Iteration:  94% 656/697 [00:35<00:02, 17.65it/s]\u001b[A\n",
            "Iteration:  95% 659/697 [00:35<00:02, 18.49it/s]\u001b[A\n",
            "Iteration:  95% 661/697 [00:35<00:02, 17.67it/s]\u001b[A\n",
            "Iteration:  95% 664/697 [00:36<00:01, 18.48it/s]\u001b[A\n",
            "Iteration:  96% 666/697 [00:36<00:01, 17.74it/s]\u001b[A\n",
            "Iteration:  96% 669/697 [00:36<00:01, 18.57it/s]\u001b[A\n",
            "Iteration:  96% 671/697 [00:36<00:01, 17.81it/s]\u001b[A\n",
            "Iteration:  97% 674/697 [00:36<00:01, 18.54it/s]\u001b[A\n",
            "Iteration:  97% 676/697 [00:36<00:01, 17.71it/s]\u001b[A\n",
            "Iteration:  97% 679/697 [00:36<00:00, 18.47it/s]\u001b[A\n",
            "Iteration:  98% 681/697 [00:37<00:00, 17.63it/s]\u001b[A\n",
            "Iteration:  98% 684/697 [00:37<00:00, 18.46it/s]\u001b[A\n",
            "Iteration:  98% 686/697 [00:37<00:00, 17.69it/s]\u001b[A\n",
            "Iteration:  99% 689/697 [00:37<00:00, 18.47it/s]\u001b[A\n",
            "Iteration:  99% 691/697 [00:37<00:00, 17.70it/s]\u001b[A\n",
            "Iteration: 100% 694/697 [00:37<00:00, 18.48it/s]\u001b[A\n",
            "Iteration: 100% 697/697 [00:37<00:00, 18.39it/s]\n",
            "Epoch: 100% 1/1 [00:37<00:00, 37.91s/it]\n",
            "06/19/2020 19:05:01 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "06/19/2020 19:05:01 - INFO - transformers.trainer -   Saving model checkpoint to /content/drive/My Drive/NLP/distilgpt2\n",
            "06/19/2020 19:05:01 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/NLP/distilgpt2/config.json\n",
            "06/19/2020 19:05:02 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/NLP/distilgpt2/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kor0khwpEtfg",
        "colab_type": "text"
      },
      "source": [
        "# Model Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXcPKJCzKAQL",
        "colab_type": "text"
      },
      "source": [
        "## Test Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ans0PLLSlt_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "003a80d6-980c-42ad-9bfc-1f8a5b7b81c7"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import random\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/NLP/distilgpt2/\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"/content/drive/My Drive/NLP/distilgpt2/\")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:774: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTHsd9Ng-f-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d68e84df-1414-40ee-c9b2-b214e5dfbdde"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "prompt = 'This bra '\n",
        "input_ids = tokenizer.encode(\n",
        "    prompt, \n",
        "    add_special_tokens=False, \n",
        "    return_tensors=\"pt\", \n",
        "    add_space_before_punct_symbol=True\n",
        "    )\n",
        "\n",
        "print(prompt)\n",
        "print(input_ids[0])"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This bra \n",
            "tensor([1212, 8290])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L18ahP9cFIAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "0bdcfccc-1c00-495a-a95a-3cb14d4efce2"
      },
      "source": [
        "# set top_k = 50 and set top_p = 0.95 and  num_return_sequences = 3 and no_repeat_ngram_size = 2\n",
        "sample_outputs = model.generate(\n",
        "    # config params\n",
        "    input_ids=input_ids,\n",
        "    max_length=30,\n",
        "    min_length=10,\n",
        "    num_return_sequences=5,\n",
        "    \n",
        "    # randomely pick next word\n",
        "    do_sample=True, \n",
        "    \n",
        "    # limiting next words\n",
        "    top_k=10, # possible orders\n",
        "    top_p=0.9 # probability of words\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"\\n{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "0: This bra from the Calvin Klein Collection. style: 1792-5447 | color: 678 Soft stretch lace waistband Full coverage Real Soft stretch\n",
            "\n",
            "1: This bra that we call the best-selling in the whole thing!\n",
            "The perfect fit to your bra.\n",
            "This bra is made with the right\n",
            "\n",
            "2: This bra is designed with a soft, light, and flattering silhouette that adds support to your bra.   The color of this bra features a light,\n",
            "\n",
            "3: This bra has a pretty lace lining, and a low rise under the waistband. This bra is made for comfort and a lot of love.\n",
            "\n",
            "\n",
            "4: This bra with a low rise waistband and a low rise waistband. This style is perfect for the occasion you're looking for. The lace cups\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXoMyn0PJrqz",
        "colab_type": "text"
      },
      "source": [
        "## Test Non-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRXNrX_2JZ1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f9f9e0a5-65f3-48cc-f5bb-6d7a8df58870"
      },
      "source": [
        "default_tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "default_model = AutoModelWithLMHead.from_pretrained(\"distilgpt2\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:774: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt9duMucn_qh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "465c00d5-6886-4613-c0ef-1bf70b06bbec"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "prompt = 'This bra '\n",
        "input_ids = default_tokenizer.encode(\n",
        "    prompt, \n",
        "    add_special_tokens=False, \n",
        "    return_tensors=\"pt\", \n",
        "    add_space_before_punct_symbol=True\n",
        "    )\n",
        "\n",
        "print(prompt)\n",
        "print(input_ids[0])"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This bra \n",
            "tensor([1212, 8290])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4fVdmWeJdCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "e33c82a7-a1dd-41f3-efbf-3d6e35c45632"
      },
      "source": [
        "# set top_k = 50 and set top_p = 0.95 and  num_return_sequences = 3 and no_repeat_ngram_size = 2\n",
        "sample_outputs = default_model.generate(\n",
        "    # config params\n",
        "    input_ids=input_ids,\n",
        "    max_length=30,\n",
        "    min_length=10,\n",
        "    num_return_sequences=5,\n",
        "    \n",
        "    # randomely pick next word\n",
        "    do_sample=True, \n",
        "    \n",
        "    # limiting next words\n",
        "    top_k=10, # possible orders\n",
        "    top_p=0.9 # probability of words\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"\\n{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "0: This bra, and you‪re ready to go!!”\n",
            "\n",
            "“It‪s so cute!”\n",
            "\n",
            "1: This bra that I have always loved. The bra that I have always loved. The bra that I have always loved. The bra that I have always\n",
            "\n",
            "2: This bra is a great choice of size and I am going to be making a new bra in the next few weeks. I have never tried a bra\n",
            "\n",
            "3: This bra in this one!\n",
            "\n",
            "This bra is a little too light, but it's a nice touch and I can easily make this bra even\n",
            "\n",
            "4: This bra and it has a lot of fun.\n",
            "\n",
            "\n",
            "\n",
            "I've been wanting this bra for a couple months now, and the only thing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjpA2ESuJnLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}